{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c070c66",
   "metadata": {},
   "source": [
    "# Create Annotation File from Extraction JSON\n",
    "\n",
    "This notebook reads extraction JSON files from parent folder and creates simplified annotation file containing only compositions and their properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe73bba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10 file(s). Processing...\n",
      "\n",
      "[OK]  mech_13_alloys_and_compounds\\mech_13_alloys_and_compounds_extraction.json  -->  annotation_mech_13_alloys_and_compounds_extraction.json\n",
      "[OK]  mech_1_physical_review_b\\mech_1_physical_review_b_extraction.json  -->  annotation_mech_1_physical_review_b_extraction.json\n",
      "[OK]  mech_3_acta_matarialia\\mech_3_acta_matarialia_extraction.json  -->  annotation_mech_3_acta_matarialia_extraction.json\n",
      "[OK]  mech_6_alloys_and_compounds\\mech_6_alloys_and_compounds_extraction.json  -->  annotation_mech_6_alloys_and_compounds_extraction.json\n",
      "[OK]  mech_9_physical_review_b\\mech_9_physical_review_b_extraction.json  -->  annotation_mech_9_physical_review_b_extraction.json\n",
      "[OK]  thermo_12_j_apl_phy\\thermo_12_j_apl_phy_extraction.json  -->  annotation_thermo_12_j_apl_phy_extraction.json\n",
      "[OK]  thermo_2_mat_aci_eng_b\\thermo_2_mat_aci_eng_b_extraction.json  -->  annotation_thermo_2_mat_aci_eng_b_extraction.json\n",
      "[OK]  thermo_4_alloys_and_compounds\\thermo_4_alloys_and_compounds_extraction.json  -->  annotation_thermo_4_alloys_and_compounds_extraction.json\n",
      "[OK]  thermo_5_j_apl_phy\\thermo_5_j_apl_phy_extraction.json  -->  annotation_thermo_5_j_apl_phy_extraction.json\n",
      "[OK]  thermo_9_alloys_and_compounds\\thermo_9_alloys_and_compounds_extraction.json  -->  annotation_thermo_9_alloys_and_compounds_extraction.json\n",
      "\n",
      "Done.\n",
      "Successful: 10\n",
      "Failed:     0\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# === Set this to your top-level folder ===\n",
    "BASE_DIR = Path(r\"C:\\Users\\hsayeed\\Documents\\GitHub\\KnowMat2\\data\\raw\\New\\Hasan\")\n",
    "\n",
    "def filter_properties(record: dict) -> dict:\n",
    "    \"\"\"Keep compositions, but filter properties to those with a non-empty standard_property_name.\"\"\"\n",
    "    out = {\"compositions\": []}\n",
    "    for comp in record.get(\"compositions\", []):\n",
    "        props = comp.get(\"properties_of_composition\", [])\n",
    "        filtered = [p for p in props if p.get(\"standard_property_name\")]\n",
    "        out[\"compositions\"].append({\n",
    "            \"composition\": comp.get(\"composition\", \"\"),\n",
    "            \"properties_of_composition\": filtered\n",
    "        })\n",
    "    return out\n",
    "\n",
    "def process_extraction_file(src_path: Path) -> Path | None:\n",
    "    \"\"\"Process a single *_extraction.json file and write annotation_*.json next to it.\"\"\"\n",
    "    try:\n",
    "        with src_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to read {src_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "    filtered = filter_properties(data)\n",
    "\n",
    "    # Output name starts with 'annotation_' and stays in the same folder\n",
    "    out_path = src_path.with_name(f\"annotation_{src_path.stem}{src_path.suffix}\")\n",
    "\n",
    "    try:\n",
    "        with out_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(filtered, f, indent=2, ensure_ascii=False)\n",
    "        return out_path\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to write {out_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def process_folder(base_dir: Path):\n",
    "    files = list(base_dir.rglob(\"*_extraction.json\"))\n",
    "    if not files:\n",
    "        print(f\"No *_extraction.json files found under: {base_dir}\")\n",
    "        return\n",
    "\n",
    "    print(f\"Found {len(files)} file(s). Processing...\\n\")\n",
    "    success, failed = 0, 0\n",
    "    for fp in files:\n",
    "        out = process_extraction_file(fp)\n",
    "        if out:\n",
    "            success += 1\n",
    "            print(f\"[OK]  {fp.relative_to(base_dir)}  -->  {out.name}\")\n",
    "        else:\n",
    "            failed += 1\n",
    "\n",
    "    print(\"\\nDone.\")\n",
    "    print(f\"Successful: {success}\")\n",
    "    print(f\"Failed:     {failed}\")\n",
    "\n",
    "# Run it\n",
    "process_folder(BASE_DIR)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "knowmat_for_agentic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
